{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993a3075",
   "metadata": {},
   "source": [
    "## Freeze layers of a model\n",
    "- You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  \n",
    "    # Check for first layer's weight\n",
    "    if name == '0.weight':\n",
    "   \n",
    "        # Freeze this weight\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Check for second layer's weight\n",
    "    if name == '1.weight':\n",
    "      \n",
    "        # Freeze this weight\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca4e4d",
   "metadata": {},
   "source": [
    "## Layer initialization\n",
    "- The initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.\n",
    "\n",
    "- As a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer's weights with the uniform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd6d51",
   "metadata": {},
   "source": [
    "## Evaluation Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df035015",
   "metadata": {},
   "source": [
    "## Writing the evaluation loop\n",
    "- In this exercise, you will write an evaluation loop to compute validation loss. The evaluation loop follows a similar structure to the training loop but without gradient calculations or weight updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for features, labels in validationloader:\n",
    "      outputs = model(features)\n",
    "      loss = criterion(outputs, labels)\n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss / len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37a4f4",
   "metadata": {},
   "source": [
    "## Calculating accuracy using torchmetrics\n",
    "- Tracking accuracy during training helps identify the best-performing epoch.\n",
    "\n",
    "- In this exercise, you'll use torchmetrics to calculate accuracy on a facemask dataset with three classes. The plot_errors function will highlight misclassified samples, helping you analyze model errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy metric\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for features, labels in dataloader:\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    metric.update(outputs, labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "accuracy = metric.compute()\n",
    "print(f\"Accuracy on all data: {accuracy}\")\n",
    "\n",
    "# Reset metric for the next epoch\n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5ad34",
   "metadata": {},
   "source": [
    "## Fighting overfiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a533369",
   "metadata": {},
   "source": [
    "## Experimenting with dropout\n",
    "- Dropout helps prevent overfitting by randomly setting some output values to zero during training. In this exercise, you'll build a simple neural network with dropout and observe how it behaves in training and evaluation modes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2597e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 6),\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Dropout(p=0.5))\n",
    "\n",
    "model.train()\n",
    "output_train = model(features)\n",
    "\n",
    "# Forward pass in evaluation mode (Dropout disabled)\n",
    "model.eval()\n",
    "output_eval = model(features)\n",
    "\n",
    "# Print results\n",
    "print(\"Output in train mode:\", output_train)\n",
    "print(\"Output in eval mode:\", output_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b0575",
   "metadata": {},
   "source": [
    "## Implementing random search\n",
    "- Hyperparameter search is a computationally costly approach to experiment with different hyperparameter values. However, it can lead to performance improvements. In this exercise, you will implement a random search algorithm.\n",
    "\n",
    "- You will randomly sample 10 values of the learning rate and momentum from the uniform distribution. To do so, you will use the np.random.uniform() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor between 2 and 4\n",
    "    factor = np.random.uniform(2, 4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly select a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))\n",
    "       \n",
    "plot_hyperparameter_search(values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
